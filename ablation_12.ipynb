{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a943e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿GPU visible?: True\n",
      "¿GPU visible?: True\n",
      "torch.cuda.is_available -> True\n",
      "torch.version.cuda      -> 11.8\n",
      "cupy GPU visible        -> True\n",
      "[INFO] Lista de 131 ROIs guardada en: ./resultados_12_inter/roi_order_131.joblib\n",
      "fatal: no es un repositorio git (ni ninguno de los directorios superiores): .git\n",
      "[INFO] Git commit hash: N/A\n",
      "[INFO] --- Configuración de la Ejecución (v1.7.0) ---\n",
      "[INFO] batch_size: 64\n",
      "[INFO] beta_vae: 4.6\n",
      "[INFO] channels_to_use: [1, 2, 5]\n",
      "[INFO] classifier_calibrate: True\n",
      "[INFO] classifier_hp_tune_ratio: 0.25\n",
      "[INFO] classifier_stratify_cols: ['Sex']\n",
      "[INFO] classifier_types: ['xgb', 'svm', 'logreg', 'gb']\n",
      "[INFO] classifier_use_class_weight: True\n",
      "[INFO] cyclical_beta_n_cycles: 4\n",
      "[INFO] cyclical_beta_ratio_increase: 0.4\n",
      "[INFO] decoder_type: convtranspose\n",
      "[INFO] dropout_rate_vae: 0.2\n",
      "[INFO] early_stopping_patience_vae: 30\n",
      "[INFO] epochs_vae: 300\n",
      "[INFO] git_hash: N/A\n",
      "[INFO] global_tensor_path: /home/diego/Escritorio/limpio/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz\n",
      "[INFO] gridsearch_scoring: roc_auc\n",
      "[INFO] inner_folds: 5\n",
      "[INFO] intermediate_fc_dim_vae: quarter\n",
      "[INFO] latent_dim: 512\n",
      "[INFO] latent_features_type: mu\n",
      "[INFO] log_interval_epochs_vae: 5\n",
      "[INFO] lr_scheduler_T0: 30\n",
      "[INFO] lr_scheduler_eta_min: 1e-07\n",
      "[INFO] lr_scheduler_patience_vae: 15\n",
      "[INFO] lr_scheduler_type: cosine_warm\n",
      "[INFO] lr_vae: 0.0001\n",
      "[INFO] metadata_features: ['Age', 'Sex']\n",
      "[INFO] metadata_path: /home/diego/Escritorio/limpio/SubjectsData_AAL3_procesado.csv\n",
      "[INFO] mlp_classifier_hidden_layers: 64,16\n",
      "[INFO] n_jobs_gridsearch: 8\n",
      "[INFO] norm_mode: zscore_offdiag\n",
      "[INFO] num_conv_layers_encoder: 4\n",
      "[INFO] num_workers: 4\n",
      "[INFO] outer_folds: 5\n",
      "[INFO] output_dir: ./resultados_12_inter\n",
      "[INFO] repeated_outer_folds_n_repeats: 1\n",
      "[INFO] roi_order_path: /home/diego/Escritorio/limpio/roi_order_131.npy\n",
      "[INFO] save_fold_artefacts: True\n",
      "[INFO] save_vae_training_history: True\n",
      "[INFO] seed: 42\n",
      "[INFO] tune_sampler_params: False\n",
      "[INFO] use_layernorm_vae_fc: False\n",
      "[INFO] use_optuna_pruner: True\n",
      "[INFO] use_smote: True\n",
      "[INFO] vae_final_activation: tanh\n",
      "[INFO] vae_val_split_ratio: 0.2\n",
      "[INFO] weight_decay_vae: 1e-05\n",
      "[INFO] ------------------------------------\n",
      "[INFO] Cargando tensor global desde: /home/diego/Escritorio/limpio/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz\n",
      "[INFO] Tensor global cargado. Forma: (431, 7, 131, 131)\n",
      "[INFO] Cargando metadatos desde: /home/diego/Escritorio/limpio/SubjectsData_AAL3_procesado.csv\n",
      "[INFO] Metadatos cargados. Forma: (434, 32)\n",
      "[INFO] Usando canales seleccionados (índices): [1, 2, 5]\n",
      "[INFO] Nombres de canales seleccionados: ['Pearson_Full_FisherZ_Signed', 'MI_KNN_Symmetric', 'DistanceCorr']\n",
      "[INFO] Estratificando folds del CLASIFICADOR por: ['ResearchGroup_Mapped', 'Sex']\n",
      "[INFO] Sujetos CN/AD para clasificación: 184. CN: 89, AD: 95\n",
      "[INFO] Usando CV externa: StratifiedKFold con 5 iteraciones totales.\n",
      "[INFO] --- Iniciando Fold 1/5 ---\n",
      "[INFO] Fold 1/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 19 (51.4%)\n",
      "      M: 18 (48.6%)\n",
      "[INFO] Fold 1/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 187 (47.5%)\n",
      "      M: 207 (52.5%)\n",
      "    Age_Group:\n",
      "      0: 104 (26.4%)\n",
      "      1: 102 (25.9%)\n",
      "      2: 91 (23.1%)\n",
      "      3: 97 (24.6%)\n",
      "[INFO]   Fold 1/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 1/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 62 (19.7%)\n",
      "      CN: 56 (17.8%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 149 (47.3%)\n",
      "      M: 166 (52.7%)\n",
      "    Age_Group:\n",
      "      0: 84 (26.7%)\n",
      "      1: 81 (25.7%)\n",
      "      2: 73 (23.2%)\n",
      "      3: 77 (24.4%)\n",
      "[INFO] Fold 1/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 14 (17.7%)\n",
      "      CN: 15 (19.0%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 38 (48.1%)\n",
      "      M: 41 (51.9%)\n",
      "    Age_Group:\n",
      "      0: 20 (25.3%)\n",
      "      1: 21 (26.6%)\n",
      "      2: 18 (22.8%)\n",
      "      3: 20 (25.3%)\n",
      "[INFO]   Fold 1/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.046, std=0.777)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.059, std=0.815)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.397, std=1.627)\n",
      "[INFO]   Fold 1/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 1/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 1/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 1/5 VAE E5/300, TrL: 63609.38 (R: 63500.62, KLD: 177.33), Beta: 0.613, LR: 9.38e-05, ValL: 62479.71 (R: 62426.25, KLD: 87.16)\n",
      "[INFO]   Fold 1/5 VAE E10/300, TrL: 59618.39 (R: 59195.74, KLD: 306.27), Beta: 1.380, LR: 7.59e-05, ValL: 57470.59 (R: 56819.64, KLD: 471.71)\n",
      "[INFO]   Fold 1/5 VAE E15/300, TrL: 53945.98 (R: 53050.01, KLD: 417.38), Beta: 2.147, LR: 5.11e-05, ValL: 53043.29 (R: 52033.89, KLD: 470.22)\n",
      "[INFO]   Fold 1/5 VAE E20/300, TrL: 52023.85 (R: 51215.70, KLD: 277.40), Beta: 2.913, LR: 2.60e-05, ValL: 51209.39 (R: 50469.52, KLD: 253.96)\n",
      "[INFO]   Fold 1/5 VAE E25/300, TrL: 51462.32 (R: 50542.83, KLD: 249.86), Beta: 3.680, LR: 7.32e-06, ValL: 50701.12 (R: 49857.17, KLD: 229.33)\n",
      "[INFO]   Fold 1/5 VAE E30/300, TrL: 51549.20 (R: 50463.54, KLD: 244.15), Beta: 4.447, LR: 1.11e-07, ValL: 50750.93 (R: 49762.37, KLD: 222.32)\n",
      "[INFO]   Fold 1/5 VAE E35/300, TrL: 48625.33 (R: 47801.38, KLD: 179.12), Beta: 4.600, LR: 9.38e-05, ValL: 47060.64 (R: 46296.21, KLD: 166.18)\n",
      "[INFO]   Fold 1/5 VAE E40/300, TrL: 45995.88 (R: 45310.99, KLD: 148.89), Beta: 4.600, LR: 7.59e-05, ValL: 44148.84 (R: 43511.99, KLD: 138.44)\n",
      "[INFO]   Fold 1/5 VAE E45/300, TrL: 44161.47 (R: 43501.36, KLD: 143.50), Beta: 4.600, LR: 5.11e-05, ValL: 42334.23 (R: 41739.17, KLD: 129.36)\n",
      "[INFO]   Fold 1/5 VAE E50/300, TrL: 43209.03 (R: 42552.68, KLD: 142.69), Beta: 4.600, LR: 2.60e-05, ValL: 41571.35 (R: 41000.52, KLD: 124.09)\n",
      "[INFO]   Fold 1/5 VAE E55/300, TrL: 42843.22 (R: 42209.22, KLD: 137.83), Beta: 4.600, LR: 7.32e-06, ValL: 41200.56 (R: 40675.16, KLD: 114.22)\n",
      "[INFO]   Fold 1/5 VAE E60/300, TrL: 42732.36 (R: 42108.29, KLD: 135.67), Beta: 4.600, LR: 1.11e-07, ValL: 41077.71 (R: 40572.29, KLD: 109.87)\n",
      "[INFO]   Fold 1/5 VAE E65/300, TrL: 41563.71 (R: 40914.13, KLD: 141.21), Beta: 4.600, LR: 9.38e-05, ValL: 39746.44 (R: 39243.96, KLD: 109.24)\n",
      "[INFO]   Fold 1/5 VAE E70/300, TrL: 40452.52 (R: 39826.86, KLD: 136.01), Beta: 4.600, LR: 7.59e-05, ValL: 38891.41 (R: 38379.79, KLD: 111.22)\n",
      "[INFO]   Fold 1/5 VAE E75/300, TrL: 39747.17 (R: 39122.50, KLD: 135.80), Beta: 4.600, LR: 5.11e-05, ValL: 38244.83 (R: 37771.19, KLD: 102.96)\n",
      "[INFO]   Fold 1/5 VAE E80/300, TrL: 38681.47 (R: 38572.18, KLD: 178.18), Beta: 0.613, LR: 2.60e-05, ValL: 37511.22 (R: 37421.27, KLD: 146.65)\n",
      "[INFO]   Fold 1/5 VAE E85/300, TrL: 38772.31 (R: 38512.44, KLD: 188.31), Beta: 1.380, LR: 7.32e-06, ValL: 37448.17 (R: 37235.94, KLD: 153.79)\n",
      "[INFO]   Fold 1/5 VAE E90/300, TrL: 38788.68 (R: 38388.82, KLD: 186.27), Beta: 2.147, LR: 1.11e-07, ValL: 37505.23 (R: 37169.72, KLD: 156.30)\n",
      "[INFO]   Fold 1/5 VAE E95/300, TrL: 38355.58 (R: 37843.89, KLD: 175.64), Beta: 2.913, LR: 9.38e-05, ValL: 37072.10 (R: 36647.76, KLD: 145.65)\n",
      "[INFO]   Fold 1/5 VAE E100/300, TrL: 37915.00 (R: 37263.81, KLD: 176.95), Beta: 3.680, LR: 7.59e-05, ValL: 36750.29 (R: 36203.88, KLD: 148.48)\n",
      "[INFO]   Fold 1/5 VAE E105/300, TrL: 37547.54 (R: 36869.52, KLD: 152.48), Beta: 4.447, LR: 5.11e-05, ValL: 36280.82 (R: 35730.49, KLD: 123.76)\n",
      "[INFO]   Fold 1/5 VAE E110/300, TrL: 37438.28 (R: 36789.65, KLD: 141.01), Beta: 4.600, LR: 2.60e-05, ValL: 36154.62 (R: 35642.60, KLD: 111.31)\n",
      "[INFO]   Fold 1/5 VAE E115/300, TrL: 37258.50 (R: 36603.87, KLD: 142.31), Beta: 4.600, LR: 7.32e-06, ValL: 36017.00 (R: 35507.70, KLD: 110.72)\n",
      "[INFO]   Fold 1/5 VAE E120/300, TrL: 37171.35 (R: 36531.78, KLD: 139.04), Beta: 4.600, LR: 1.11e-07, ValL: 35956.60 (R: 35456.63, KLD: 108.69)\n",
      "[INFO]   Fold 1/5 VAE E125/300, TrL: 36949.27 (R: 36282.90, KLD: 144.86), Beta: 4.600, LR: 9.38e-05, ValL: 35619.23 (R: 35080.11, KLD: 117.20)\n",
      "[INFO]   Fold 1/5 VAE E130/300, TrL: 36428.68 (R: 35765.29, KLD: 144.22), Beta: 4.600, LR: 7.59e-05, ValL: 35273.67 (R: 34732.71, KLD: 117.60)\n",
      "[INFO]   Fold 1/5 VAE E135/300, TrL: 36136.82 (R: 35496.03, KLD: 139.30), Beta: 4.600, LR: 5.11e-05, ValL: 35008.11 (R: 34486.76, KLD: 113.34)\n",
      "[INFO]   Fold 1/5 VAE E140/300, TrL: 35944.58 (R: 35291.92, KLD: 141.88), Beta: 4.600, LR: 2.60e-05, ValL: 34835.11 (R: 34314.98, KLD: 113.07)\n",
      "[INFO]   Fold 1/5 VAE E145/300, TrL: 35968.88 (R: 35326.01, KLD: 139.75), Beta: 4.600, LR: 7.32e-06, ValL: 34750.13 (R: 34237.11, KLD: 111.53)\n",
      "[INFO]   Fold 1/5 VAE E150/300, TrL: 35800.57 (R: 35159.68, KLD: 139.33), Beta: 4.600, LR: 1.11e-07, ValL: 34767.92 (R: 34260.33, KLD: 110.35)\n",
      "[INFO]   Fold 1/5 VAE E155/300, TrL: 34860.85 (R: 34724.38, KLD: 222.49), Beta: 0.613, LR: 9.38e-05, ValL: 34021.73 (R: 33900.83, KLD: 197.12)\n",
      "[INFO]   Fold 1/5 VAE E160/300, TrL: 34744.14 (R: 34409.01, KLD: 242.85), Beta: 1.380, LR: 7.59e-05, ValL: 33846.18 (R: 33552.00, KLD: 213.17)\n",
      "[INFO]   Fold 1/5 VAE E165/300, TrL: 34575.34 (R: 34090.71, KLD: 225.76), Beta: 2.147, LR: 5.11e-05, ValL: 33800.59 (R: 33379.96, KLD: 195.94)\n",
      "[INFO]   Fold 1/5 VAE E170/300, TrL: 34651.73 (R: 34044.15, KLD: 208.55), Beta: 2.913, LR: 2.60e-05, ValL: 33897.66 (R: 33376.43, KLD: 178.91)\n",
      "[INFO]   Fold 1/5 VAE E175/300, TrL: 34655.56 (R: 33929.71, KLD: 197.24), Beta: 3.680, LR: 7.32e-06, ValL: 33883.59 (R: 33267.99, KLD: 167.28)\n",
      "[INFO]   Fold 1/5 VAE E180/300, TrL: 34625.50 (R: 33763.49, KLD: 193.85), Beta: 4.447, LR: 1.11e-07, ValL: 34037.25 (R: 33303.19, KLD: 165.08)\n",
      "[INFO]   Fold 1/5 VAE E185/300, TrL: 34665.53 (R: 33969.40, KLD: 151.33), Beta: 4.600, LR: 9.38e-05, ValL: 33908.13 (R: 33283.74, KLD: 135.74)\n",
      "[INFO]   Fold 1/5 VAE E190/300, TrL: 34360.94 (R: 33652.28, KLD: 154.06), Beta: 4.600, LR: 7.59e-05, ValL: 33579.66 (R: 32992.87, KLD: 127.56)\n",
      "[INFO]   Fold 1/5 VAE E195/300, TrL: 34164.23 (R: 33445.02, KLD: 156.35), Beta: 4.600, LR: 5.11e-05, ValL: 33482.26 (R: 32899.05, KLD: 126.79)\n",
      "[INFO]   Fold 1/5 VAE E200/300, TrL: 34050.26 (R: 33362.50, KLD: 149.51), Beta: 4.600, LR: 2.60e-05, ValL: 33284.32 (R: 32731.50, KLD: 120.18)\n",
      "[INFO]   Fold 1/5 VAE E205/300, TrL: 34046.58 (R: 33364.01, KLD: 148.39), Beta: 4.600, LR: 7.32e-06, ValL: 33304.66 (R: 32763.66, KLD: 117.61)\n",
      "[INFO]   Fold 1/5 VAE E210/300, TrL: 33922.10 (R: 33245.41, KLD: 147.11), Beta: 4.600, LR: 1.11e-07, ValL: 33273.11 (R: 32734.68, KLD: 117.05)\n",
      "[INFO]   Fold 1/5 VAE E215/300, TrL: 33810.45 (R: 33112.93, KLD: 151.63), Beta: 4.600, LR: 9.38e-05, ValL: 33207.59 (R: 32636.88, KLD: 124.07)\n",
      "[INFO]   Fold 1/5 VAE E220/300, TrL: 33703.93 (R: 32990.66, KLD: 155.06), Beta: 4.600, LR: 7.59e-05, ValL: 33045.87 (R: 32471.05, KLD: 124.96)\n",
      "[INFO]   Fold 1/5 VAE E225/300, TrL: 33517.13 (R: 32823.43, KLD: 150.80), Beta: 4.600, LR: 5.11e-05, ValL: 32859.62 (R: 32299.30, KLD: 121.81)\n",
      "[INFO]   Fold 1/5 VAE E230/300, TrL: 32781.96 (R: 32672.24, KLD: 178.89), Beta: 0.613, LR: 2.60e-05, ValL: 32231.14 (R: 32137.52, KLD: 152.63)\n",
      "[INFO]   Fold 1/5 VAE E235/300, TrL: 32698.84 (R: 32434.14, KLD: 191.81), Beta: 1.380, LR: 7.32e-06, ValL: 32372.00 (R: 32146.81, KLD: 163.18)\n",
      "[INFO]   Fold 1/5 VAE E240/300, TrL: 32812.19 (R: 32399.19, KLD: 192.39), Beta: 2.147, LR: 1.11e-07, ValL: 32490.31 (R: 32134.97, KLD: 165.53)\n",
      "[INFO]   Fold 1/5 VAE E245/300, TrL: 32965.90 (R: 32394.64, KLD: 196.09), Beta: 2.913, LR: 9.38e-05, ValL: 32543.29 (R: 32066.81, KLD: 163.55)\n",
      "[INFO]   Fold 1/5 VAE E250/300, TrL: 33025.46 (R: 32344.83, KLD: 184.95), Beta: 3.680, LR: 7.59e-05, ValL: 32532.06 (R: 31964.34, KLD: 154.27)\n",
      "[INFO]   Fold 1/5 VAE E255/300, TrL: 32875.76 (R: 32112.61, KLD: 171.62), Beta: 4.447, LR: 5.11e-05, ValL: 32513.46 (R: 31885.22, KLD: 141.29)\n",
      "[INFO]   Fold 1/5 Early stopping VAE en epoch 260. Mejor val_loss: 32231.1351 (época 230)\n",
      "[INFO]   Fold 1/5 VAE final model loaded (best val_loss: 32231.1351).\n",
      "[INFO]   Fold 1/5 Modelo VAE guardado en: resultados_12_inter/fold_1/vae_model_fold_1.pt\n",
      "[INFO] Fold 1/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 73 (49.7%)\n",
      "      M: 74 (50.3%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'xgb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 0.5981730330721846, 'model__n_estimators': 982, 'model__learning_rate': 0.004912189747877727, 'model__max_depth': 8, 'model__subsample': 0.9240499471543981, 'model__colsample_bytree': 0.7488322003240284, 'model__min_child_weight': 0.8058024160957681}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 1 (xgb): AUC=0.9035, Bal.Acc=0.7807\n",
      "[INFO]       Pipeline completo de xgb del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'svm'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 2501.815623264189, 'model__estimator__gamma': 1.1991269967976865e-06, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 1 (svm): AUC=0.9123, Bal.Acc=0.8377\n",
      "[INFO]       Pipeline completo de svm del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'logreg'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.002519711526382572}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 1 (logreg): AUC=0.8830, Bal.Acc=0.7822\n",
      "[INFO]       Pipeline completo de logreg del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'gb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 8, 'model__estimator__num_leaves': 180, 'model__estimator__bagging_fraction': 0.6214582789085462, 'model__estimator__feature_fraction': 0.692592234424208, 'model__estimator__bagging_freq': 4, 'model__estimator__learning_rate': 0.012167407470797545, 'model__estimator__n_estimators': 799, 'model__estimator__min_child_samples': 10, 'model__estimator__min_child_weight': 1.9220375013720374, 'model__estimator__min_split_gain': 0.34397655618224743, 'model__estimator__reg_alpha': 0.361200104439352, 'model__estimator__reg_lambda': 0.48165823993451945}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 1 (gb): AUC=0.9313, Bal.Acc=0.8933\n",
      "[INFO]       Pipeline completo de gb del fold 1 guardado.\n",
      "[INFO]   Fold 1/5 completado en 327.49 segundos.\n",
      "[INFO] --- Iniciando Fold 2/5 ---\n",
      "[INFO] Fold 2/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 19 (51.4%)\n",
      "      M: 18 (48.6%)\n",
      "[INFO] Fold 2/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 187 (47.5%)\n",
      "      M: 207 (52.5%)\n",
      "    Age_Group:\n",
      "      0: 98 (24.9%)\n",
      "      1: 101 (25.6%)\n",
      "      2: 94 (23.9%)\n",
      "      3: 101 (25.6%)\n",
      "[INFO]   Fold 2/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 2/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 61 (19.4%)\n",
      "      CN: 57 (18.1%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 150 (47.6%)\n",
      "      M: 165 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 79 (25.1%)\n",
      "      1: 81 (25.7%)\n",
      "      2: 75 (23.8%)\n",
      "      3: 80 (25.4%)\n",
      "[INFO] Fold 2/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 15 (19.0%)\n",
      "      CN: 14 (17.7%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 37 (46.8%)\n",
      "      M: 42 (53.2%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 20 (25.3%)\n",
      "      2: 19 (24.1%)\n",
      "      3: 21 (26.6%)\n",
      "[INFO]   Fold 2/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.049, std=0.776)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.055, std=0.813)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.408, std=1.730)\n",
      "[INFO]   Fold 2/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 2/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 2/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 2/5 VAE E5/300, TrL: 63610.00 (R: 63502.48, KLD: 175.32), Beta: 0.613, LR: 9.38e-05, ValL: 57535.44 (R: 57484.97, KLD: 82.29)\n",
      "[INFO]   Fold 2/5 VAE E10/300, TrL: 59904.03 (R: 59520.78, KLD: 277.71), Beta: 1.380, LR: 7.59e-05, ValL: 52625.19 (R: 51980.87, KLD: 466.90)\n",
      "[INFO]   Fold 2/5 VAE E15/300, TrL: 53922.78 (R: 52998.45, KLD: 430.59), Beta: 2.147, LR: 5.11e-05, ValL: 48846.35 (R: 47552.04, KLD: 602.94)\n",
      "[INFO]   Fold 2/5 VAE E20/300, TrL: 52043.45 (R: 51242.30, KLD: 275.00), Beta: 2.913, LR: 2.60e-05, ValL: 46800.17 (R: 45990.39, KLD: 277.96)\n",
      "[INFO]   Fold 2/5 VAE E25/300, TrL: 51588.70 (R: 50696.11, KLD: 242.55), Beta: 3.680, LR: 7.32e-06, ValL: 46379.00 (R: 45450.29, KLD: 252.37)\n",
      "[INFO]   Fold 2/5 VAE E30/300, TrL: 51599.43 (R: 50541.57, KLD: 237.90), Beta: 4.447, LR: 1.11e-07, ValL: 46421.56 (R: 45339.52, KLD: 243.34)\n",
      "[INFO]   Fold 2/5 VAE E35/300, TrL: 48965.00 (R: 48206.11, KLD: 164.98), Beta: 4.600, LR: 9.38e-05, ValL: 43223.31 (R: 42430.48, KLD: 172.35)\n",
      "[INFO]   Fold 2/5 VAE E40/300, TrL: 46456.08 (R: 45814.10, KLD: 139.56), Beta: 4.600, LR: 7.59e-05, ValL: 40370.67 (R: 39738.38, KLD: 137.45)\n",
      "[INFO]   Fold 2/5 VAE E45/300, TrL: 44804.21 (R: 44197.10, KLD: 131.98), Beta: 4.600, LR: 5.11e-05, ValL: 38676.95 (R: 38152.15, KLD: 114.09)\n",
      "[INFO]   Fold 2/5 VAE E50/300, TrL: 43902.48 (R: 43325.94, KLD: 125.34), Beta: 4.600, LR: 2.60e-05, ValL: 37801.85 (R: 37335.86, KLD: 101.30)\n",
      "[INFO]   Fold 2/5 VAE E55/300, TrL: 43502.13 (R: 42912.84, KLD: 128.11), Beta: 4.600, LR: 7.32e-06, ValL: 37508.09 (R: 37023.77, KLD: 105.29)\n",
      "[INFO]   Fold 2/5 VAE E60/300, TrL: 43389.10 (R: 42808.80, KLD: 126.15), Beta: 4.600, LR: 1.11e-07, ValL: 37457.20 (R: 36996.12, KLD: 100.23)\n",
      "[INFO]   Fold 2/5 VAE E65/300, TrL: 42077.80 (R: 41490.14, KLD: 127.75), Beta: 4.600, LR: 9.38e-05, ValL: 36054.69 (R: 35532.53, KLD: 113.51)\n",
      "[INFO]   Fold 2/5 VAE E70/300, TrL: 41021.58 (R: 40441.41, KLD: 126.12), Beta: 4.600, LR: 7.59e-05, ValL: 35146.23 (R: 34627.45, KLD: 112.78)\n",
      "[INFO]   Fold 2/5 VAE E75/300, TrL: 40299.78 (R: 39721.24, KLD: 125.77), Beta: 4.600, LR: 5.11e-05, ValL: 34361.07 (R: 33902.04, KLD: 99.79)\n",
      "[INFO]   Fold 2/5 VAE E80/300, TrL: 39320.75 (R: 39213.25, KLD: 175.26), Beta: 0.613, LR: 2.60e-05, ValL: 33575.00 (R: 33482.86, KLD: 150.22)\n",
      "[INFO]   Fold 2/5 VAE E85/300, TrL: 39379.23 (R: 39125.46, KLD: 183.89), Beta: 1.380, LR: 7.32e-06, ValL: 33483.37 (R: 33273.19, KLD: 152.30)\n",
      "[INFO]   Fold 2/5 VAE E90/300, TrL: 39360.84 (R: 38972.88, KLD: 180.73), Beta: 2.147, LR: 1.11e-07, ValL: 33594.34 (R: 33263.94, KLD: 153.91)\n",
      "[INFO]   Fold 2/5 VAE E95/300, TrL: 38860.55 (R: 38381.67, KLD: 164.37), Beta: 2.913, LR: 9.38e-05, ValL: 32996.23 (R: 32601.34, KLD: 135.55)\n",
      "[INFO]   Fold 2/5 VAE E100/300, TrL: 38385.45 (R: 37792.57, KLD: 161.11), Beta: 3.680, LR: 7.59e-05, ValL: 32616.01 (R: 32116.04, KLD: 135.86)\n",
      "[INFO]   Fold 2/5 VAE E105/300, TrL: 37988.04 (R: 37329.25, KLD: 148.15), Beta: 4.447, LR: 5.11e-05, ValL: 32237.52 (R: 31727.36, KLD: 114.73)\n",
      "[INFO]   Fold 2/5 VAE E110/300, TrL: 37930.75 (R: 37290.47, KLD: 139.19), Beta: 4.600, LR: 2.60e-05, ValL: 31996.98 (R: 31492.99, KLD: 109.56)\n",
      "[INFO]   Fold 2/5 VAE E115/300, TrL: 37687.30 (R: 37051.27, KLD: 138.27), Beta: 4.600, LR: 7.32e-06, ValL: 32034.70 (R: 31539.47, KLD: 107.66)\n",
      "[INFO]   Fold 2/5 VAE E120/300, TrL: 37605.24 (R: 36966.78, KLD: 138.80), Beta: 4.600, LR: 1.11e-07, ValL: 31944.48 (R: 31455.13, KLD: 106.38)\n",
      "[INFO]   Fold 2/5 VAE E125/300, TrL: 37303.76 (R: 36604.75, KLD: 151.96), Beta: 4.600, LR: 9.38e-05, ValL: 31677.18 (R: 31128.48, KLD: 119.28)\n",
      "[INFO]   Fold 2/5 VAE E130/300, TrL: 36916.25 (R: 36265.14, KLD: 141.55), Beta: 4.600, LR: 7.59e-05, ValL: 31127.52 (R: 30595.65, KLD: 115.62)\n",
      "[INFO]   Fold 2/5 VAE E135/300, TrL: 36462.49 (R: 35794.94, KLD: 145.12), Beta: 4.600, LR: 5.11e-05, ValL: 30975.65 (R: 30444.88, KLD: 115.38)\n",
      "[INFO]   Fold 2/5 VAE E140/300, TrL: 36425.54 (R: 35767.51, KLD: 143.05), Beta: 4.600, LR: 2.60e-05, ValL: 30750.36 (R: 30218.15, KLD: 115.70)\n",
      "[INFO]   Fold 2/5 VAE E145/300, TrL: 36167.63 (R: 35529.03, KLD: 138.83), Beta: 4.600, LR: 7.32e-06, ValL: 30651.20 (R: 30147.49, KLD: 109.50)\n",
      "[INFO]   Fold 2/5 VAE E150/300, TrL: 36238.01 (R: 35597.34, KLD: 139.28), Beta: 4.600, LR: 1.11e-07, ValL: 30677.85 (R: 30175.28, KLD: 109.25)\n",
      "[INFO]   Fold 2/5 VAE E155/300, TrL: 35357.49 (R: 35221.82, KLD: 221.20), Beta: 0.613, LR: 9.38e-05, ValL: 29917.83 (R: 29799.38, KLD: 193.13)\n",
      "[INFO]   Fold 2/5 VAE E160/300, TrL: 35171.13 (R: 34827.12, KLD: 249.28), Beta: 1.380, LR: 7.59e-05, ValL: 29750.83 (R: 29432.94, KLD: 230.35)\n",
      "[INFO]   Fold 2/5 VAE E165/300, TrL: 35036.55 (R: 34536.51, KLD: 232.94), Beta: 2.147, LR: 5.11e-05, ValL: 29642.15 (R: 29203.39, KLD: 204.39)\n",
      "[INFO]   Fold 2/5 VAE E170/300, TrL: 35003.56 (R: 34384.93, KLD: 212.34), Beta: 2.913, LR: 2.60e-05, ValL: 29656.78 (R: 29119.43, KLD: 184.44)\n",
      "[INFO]   Fold 2/5 VAE E175/300, TrL: 35112.49 (R: 34371.95, KLD: 201.23), Beta: 3.680, LR: 7.32e-06, ValL: 29693.70 (R: 29059.17, KLD: 172.43)\n",
      "[INFO]   Fold 2/5 VAE E180/300, TrL: 35167.98 (R: 34277.93, KLD: 200.16), Beta: 4.447, LR: 1.11e-07, ValL: 29779.99 (R: 29029.27, KLD: 168.83)\n",
      "[INFO]   Fold 2/5 VAE E185/300, TrL: 35157.34 (R: 34409.80, KLD: 162.51), Beta: 4.600, LR: 9.38e-05, ValL: 29594.33 (R: 28980.12, KLD: 133.52)\n",
      "[INFO]   Fold 2/5 VAE E190/300, TrL: 34915.20 (R: 34192.97, KLD: 157.01), Beta: 4.600, LR: 7.59e-05, ValL: 29420.44 (R: 28830.57, KLD: 128.23)\n",
      "[INFO]   Fold 2/5 VAE E195/300, TrL: 34664.00 (R: 33937.16, KLD: 158.01), Beta: 4.600, LR: 5.11e-05, ValL: 29272.26 (R: 28701.68, KLD: 124.04)\n",
      "[INFO]   Fold 2/5 VAE E200/300, TrL: 34451.44 (R: 33741.20, KLD: 154.40), Beta: 4.600, LR: 2.60e-05, ValL: 29229.57 (R: 28675.26, KLD: 120.50)\n",
      "[INFO]   Fold 2/5 VAE E205/300, TrL: 34345.19 (R: 33651.76, KLD: 150.75), Beta: 4.600, LR: 7.32e-06, ValL: 29174.64 (R: 28624.74, KLD: 119.54)\n",
      "[INFO]   Fold 2/5 VAE E210/300, TrL: 34306.76 (R: 33616.37, KLD: 150.08), Beta: 4.600, LR: 1.11e-07, ValL: 29123.15 (R: 28578.61, KLD: 118.38)\n",
      "[INFO]   Fold 2/5 VAE E215/300, TrL: 34293.70 (R: 33566.78, KLD: 158.02), Beta: 4.600, LR: 9.38e-05, ValL: 29051.43 (R: 28469.71, KLD: 126.46)\n",
      "[INFO]   Fold 2/5 VAE E220/300, TrL: 34160.00 (R: 33419.27, KLD: 161.03), Beta: 4.600, LR: 7.59e-05, ValL: 28980.30 (R: 28403.15, KLD: 125.47)\n",
      "[INFO]   Fold 2/5 VAE E225/300, TrL: 33846.85 (R: 33108.41, KLD: 160.53), Beta: 4.600, LR: 5.11e-05, ValL: 28851.47 (R: 28284.88, KLD: 123.17)\n",
      "[INFO]   Fold 2/5 VAE E230/300, TrL: 33001.61 (R: 32889.34, KLD: 183.05), Beta: 0.613, LR: 2.60e-05, ValL: 28165.65 (R: 28073.30, KLD: 150.57)\n",
      "[INFO]   Fold 2/5 VAE E235/300, TrL: 33074.22 (R: 32804.77, KLD: 195.25), Beta: 1.380, LR: 7.32e-06, ValL: 28124.92 (R: 27901.60, KLD: 161.82)\n",
      "[INFO]   Fold 2/5 VAE E240/300, TrL: 33249.49 (R: 32830.18, KLD: 195.33), Beta: 2.147, LR: 1.11e-07, ValL: 28346.35 (R: 27993.72, KLD: 164.27)\n",
      "[INFO]   Fold 2/5 VAE E245/300, TrL: 33408.20 (R: 32836.76, KLD: 196.15), Beta: 2.913, LR: 9.38e-05, ValL: 28406.80 (R: 27948.15, KLD: 157.43)\n",
      "[INFO]   Fold 2/5 VAE E250/300, TrL: 33299.74 (R: 32622.77, KLD: 183.96), Beta: 3.680, LR: 7.59e-05, ValL: 28458.43 (R: 27903.40, KLD: 150.82)\n",
      "[INFO]   Fold 2/5 VAE E255/300, TrL: 33286.67 (R: 32524.30, KLD: 171.45), Beta: 4.447, LR: 5.11e-05, ValL: 28399.16 (R: 27777.19, KLD: 139.87)\n",
      "[INFO]   Fold 2/5 Early stopping VAE en epoch 258. Mejor val_loss: 28072.3066 (época 228)\n",
      "[INFO]   Fold 2/5 VAE final model loaded (best val_loss: 28072.3066).\n",
      "[INFO]   Fold 2/5 Modelo VAE guardado en: resultados_12_inter/fold_2/vae_model_fold_2.pt\n",
      "[INFO] Fold 2/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 73 (49.7%)\n",
      "      M: 74 (50.3%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'xgb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 3.9869061027073007, 'model__n_estimators': 549, 'model__learning_rate': 0.03011524411806235, 'model__max_depth': 8, 'model__subsample': 0.344451113036872, 'model__colsample_bytree': 0.9502300329119169, 'model__min_child_weight': 0.8113317206038415}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 2 (xgb): AUC=0.8187, Bal.Acc=0.7281\n",
      "[INFO]       Pipeline completo de xgb del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'svm'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 438.74439308207644, 'model__estimator__gamma': 6.645786787006897e-07, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 2 (svm): AUC=0.7120, Bal.Acc=0.5906\n",
      "[INFO]       Pipeline completo de svm del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'logreg'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.010651539791175335}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 2 (logreg): AUC=0.8070, Bal.Acc=0.7865\n",
      "[INFO]       Pipeline completo de logreg del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'gb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 8, 'model__estimator__num_leaves': 778, 'model__estimator__bagging_fraction': 0.9604347768779635, 'model__estimator__feature_fraction': 0.7289261803817378, 'model__estimator__bagging_freq': 2, 'model__estimator__learning_rate': 0.023821992797457135, 'model__estimator__n_estimators': 529, 'model__estimator__min_child_samples': 26, 'model__estimator__min_child_weight': 0.006850812410414383, 'model__estimator__min_split_gain': 0.23050184163057247, 'model__estimator__reg_alpha': 0.01071135900966467, 'model__estimator__reg_lambda': 0.00365673540785404}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 2 (gb): AUC=0.8436, Bal.Acc=0.7266\n",
      "[INFO]       Pipeline completo de gb del fold 2 guardado.\n",
      "[INFO]   Fold 2/5 completado en 427.92 segundos.\n",
      "[INFO] --- Iniciando Fold 3/5 ---\n",
      "[INFO] Fold 3/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 18 (48.6%)\n",
      "      M: 19 (51.4%)\n",
      "[INFO] Fold 3/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 188 (47.7%)\n",
      "      M: 206 (52.3%)\n",
      "    Age_Group:\n",
      "      0: 97 (24.6%)\n",
      "      1: 104 (26.4%)\n",
      "      2: 97 (24.6%)\n",
      "      3: 96 (24.4%)\n",
      "[INFO]   Fold 3/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 3/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 60 (19.0%)\n",
      "      CN: 58 (18.4%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 150 (47.6%)\n",
      "      M: 165 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 78 (24.8%)\n",
      "      1: 83 (26.3%)\n",
      "      2: 78 (24.8%)\n",
      "      3: 76 (24.1%)\n",
      "[INFO] Fold 3/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 16 (20.3%)\n",
      "      CN: 13 (16.5%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 38 (48.1%)\n",
      "      M: 41 (51.9%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 21 (26.6%)\n",
      "      2: 19 (24.1%)\n",
      "      3: 20 (25.3%)\n",
      "[INFO]   Fold 3/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.050, std=0.780)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.054, std=0.812)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.419, std=1.767)\n",
      "[INFO]   Fold 3/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 3/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 3/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 3/5 VAE E5/300, TrL: 63397.67 (R: 63289.39, KLD: 176.54), Beta: 0.613, LR: 9.38e-05, ValL: 55866.11 (R: 55814.06, KLD: 84.86)\n",
      "[INFO]   Fold 3/5 VAE E10/300, TrL: 59564.78 (R: 59176.83, KLD: 281.12), Beta: 1.380, LR: 7.59e-05, ValL: 50982.82 (R: 50301.47, KLD: 493.73)\n",
      "[INFO]   Fold 3/5 VAE E15/300, TrL: 54188.33 (R: 53264.62, KLD: 430.30), Beta: 2.147, LR: 5.11e-05, ValL: 47965.20 (R: 46672.85, KLD: 602.02)\n",
      "[INFO]   Fold 3/5 VAE E20/300, TrL: 52480.15 (R: 51706.35, KLD: 265.61), Beta: 2.913, LR: 2.60e-05, ValL: 46196.96 (R: 45312.09, KLD: 303.73)\n",
      "[INFO]   Fold 3/5 VAE E25/300, TrL: 52039.17 (R: 51187.57, KLD: 231.41), Beta: 3.680, LR: 7.32e-06, ValL: 45833.40 (R: 44823.40, KLD: 274.46)\n",
      "[INFO]   Fold 3/5 VAE E30/300, TrL: 52066.16 (R: 51048.59, KLD: 228.84), Beta: 4.447, LR: 1.11e-07, ValL: 45934.72 (R: 44743.13, KLD: 267.98)\n",
      "[INFO]   Fold 3/5 VAE E35/300, TrL: 49314.13 (R: 48551.56, KLD: 165.78), Beta: 4.600, LR: 9.38e-05, ValL: 42812.13 (R: 41834.37, KLD: 212.56)\n",
      "[INFO]   Fold 3/5 VAE E40/300, TrL: 46511.64 (R: 45863.34, KLD: 140.93), Beta: 4.600, LR: 7.59e-05, ValL: 40161.82 (R: 39327.42, KLD: 181.39)\n",
      "[INFO]   Fold 3/5 VAE E45/300, TrL: 44737.81 (R: 44152.61, KLD: 127.22), Beta: 4.600, LR: 5.11e-05, ValL: 38362.43 (R: 37672.43, KLD: 150.00)\n",
      "[INFO]   Fold 3/5 VAE E50/300, TrL: 43864.75 (R: 43271.94, KLD: 128.87), Beta: 4.600, LR: 2.60e-05, ValL: 37512.97 (R: 36812.45, KLD: 152.29)\n",
      "[INFO]   Fold 3/5 VAE E55/300, TrL: 43341.17 (R: 42770.60, KLD: 124.04), Beta: 4.600, LR: 7.32e-06, ValL: 37108.45 (R: 36481.49, KLD: 136.29)\n",
      "[INFO]   Fold 3/5 VAE E60/300, TrL: 43378.32 (R: 42801.88, KLD: 125.31), Beta: 4.600, LR: 1.11e-07, ValL: 37087.78 (R: 36447.62, KLD: 139.16)\n",
      "[INFO]   Fold 3/5 VAE E65/300, TrL: 42042.46 (R: 41460.25, KLD: 126.57), Beta: 4.600, LR: 9.38e-05, ValL: 35668.76 (R: 34986.75, KLD: 148.26)\n",
      "[INFO]   Fold 3/5 VAE E70/300, TrL: 40883.20 (R: 40285.88, KLD: 129.85), Beta: 4.600, LR: 7.59e-05, ValL: 34676.20 (R: 33995.07, KLD: 148.07)\n",
      "[INFO]   Fold 3/5 VAE E75/300, TrL: 40057.85 (R: 39489.66, KLD: 123.52), Beta: 4.600, LR: 5.11e-05, ValL: 33971.48 (R: 33351.56, KLD: 134.77)\n",
      "[INFO]   Fold 3/5 VAE E80/300, TrL: 39124.97 (R: 39022.84, KLD: 166.52), Beta: 0.613, LR: 2.60e-05, ValL: 33046.81 (R: 32932.57, KLD: 186.26)\n",
      "[INFO]   Fold 3/5 VAE E85/300, TrL: 39076.19 (R: 38829.77, KLD: 178.56), Beta: 1.380, LR: 7.32e-06, ValL: 33095.02 (R: 32802.50, KLD: 211.97)\n",
      "[INFO]   Fold 3/5 VAE E90/300, TrL: 39204.64 (R: 38822.59, KLD: 177.97), Beta: 2.147, LR: 1.11e-07, ValL: 33208.33 (R: 32753.74, KLD: 211.76)\n",
      "[INFO]   Fold 3/5 VAE E95/300, TrL: 38610.01 (R: 38129.13, KLD: 165.06), Beta: 2.913, LR: 9.38e-05, ValL: 32817.63 (R: 32248.89, KLD: 195.22)\n",
      "[INFO]   Fold 3/5 VAE E100/300, TrL: 38269.99 (R: 37679.76, KLD: 160.39), Beta: 3.680, LR: 7.59e-05, ValL: 32324.22 (R: 31686.18, KLD: 173.38)\n",
      "[INFO]   Fold 3/5 VAE E105/300, TrL: 37826.75 (R: 37183.97, KLD: 144.55), Beta: 4.447, LR: 5.11e-05, ValL: 32000.49 (R: 31349.07, KLD: 146.50)\n",
      "[INFO]   Fold 3/5 VAE E110/300, TrL: 37649.88 (R: 37020.64, KLD: 136.79), Beta: 4.600, LR: 2.60e-05, ValL: 31713.69 (R: 31074.86, KLD: 138.88)\n",
      "[INFO]   Fold 3/5 VAE E115/300, TrL: 37540.44 (R: 36920.69, KLD: 134.73), Beta: 4.600, LR: 7.32e-06, ValL: 31653.50 (R: 31051.55, KLD: 130.86)\n",
      "[INFO]   Fold 3/5 VAE E120/300, TrL: 37457.11 (R: 36830.58, KLD: 136.20), Beta: 4.600, LR: 1.11e-07, ValL: 31637.70 (R: 31035.90, KLD: 130.82)\n",
      "[INFO]   Fold 3/5 VAE E125/300, TrL: 37125.29 (R: 36491.14, KLD: 137.86), Beta: 4.600, LR: 9.38e-05, ValL: 31358.99 (R: 30724.81, KLD: 137.87)\n",
      "[INFO]   Fold 3/5 VAE E130/300, TrL: 36798.77 (R: 36160.52, KLD: 138.75), Beta: 4.600, LR: 7.59e-05, ValL: 31024.50 (R: 30424.64, KLD: 130.40)\n",
      "[INFO]   Fold 3/5 VAE E135/300, TrL: 36504.24 (R: 35842.48, KLD: 143.86), Beta: 4.600, LR: 5.11e-05, ValL: 30612.94 (R: 30004.48, KLD: 132.27)\n",
      "[INFO]   Fold 3/5 VAE E140/300, TrL: 36322.69 (R: 35675.08, KLD: 140.78), Beta: 4.600, LR: 2.60e-05, ValL: 30627.24 (R: 30030.67, KLD: 129.69)\n",
      "[INFO]   Fold 3/5 VAE E145/300, TrL: 36120.04 (R: 35491.50, KLD: 136.64), Beta: 4.600, LR: 7.32e-06, ValL: 30536.44 (R: 29960.36, KLD: 125.23)\n",
      "[INFO]   Fold 3/5 VAE E150/300, TrL: 36097.94 (R: 35477.20, KLD: 134.94), Beta: 4.600, LR: 1.11e-07, ValL: 30419.52 (R: 29855.86, KLD: 122.53)\n",
      "[INFO]   Fold 3/5 VAE E155/300, TrL: 35196.19 (R: 35060.95, KLD: 220.51), Beta: 0.613, LR: 9.38e-05, ValL: 29609.48 (R: 29465.59, KLD: 234.61)\n",
      "[INFO]   Fold 3/5 VAE E160/300, TrL: 35055.25 (R: 34718.46, KLD: 244.05), Beta: 1.380, LR: 7.59e-05, ValL: 29565.96 (R: 29203.91, KLD: 262.35)\n",
      "[INFO]   Fold 3/5 VAE E165/300, TrL: 34983.92 (R: 34495.25, KLD: 227.64), Beta: 2.147, LR: 5.11e-05, ValL: 29575.58 (R: 29070.10, KLD: 235.47)\n",
      "[INFO]   Fold 3/5 VAE E170/300, TrL: 34859.71 (R: 34256.12, KLD: 207.18), Beta: 2.913, LR: 2.60e-05, ValL: 29536.24 (R: 28947.91, KLD: 201.94)\n",
      "[INFO]   Fold 3/5 VAE E175/300, TrL: 35053.93 (R: 34339.75, KLD: 194.07), Beta: 3.680, LR: 7.32e-06, ValL: 29624.20 (R: 28940.95, KLD: 185.67)\n",
      "[INFO]   Fold 3/5 VAE E180/300, TrL: 35223.46 (R: 34374.43, KLD: 190.94), Beta: 4.447, LR: 1.11e-07, ValL: 29707.05 (R: 28891.99, KLD: 183.30)\n",
      "[INFO]   Fold 3/5 VAE E185/300, TrL: 34970.33 (R: 34270.20, KLD: 152.20), Beta: 4.600, LR: 9.38e-05, ValL: 29549.74 (R: 28879.47, KLD: 145.71)\n",
      "[INFO]   Fold 3/5 VAE E190/300, TrL: 34685.04 (R: 33981.36, KLD: 152.97), Beta: 4.600, LR: 7.59e-05, ValL: 29333.04 (R: 28703.87, KLD: 136.78)\n",
      "[INFO]   Fold 3/5 VAE E195/300, TrL: 34446.66 (R: 33746.59, KLD: 152.19), Beta: 4.600, LR: 5.11e-05, ValL: 29097.67 (R: 28474.16, KLD: 135.55)\n",
      "[INFO]   Fold 3/5 VAE E200/300, TrL: 34294.37 (R: 33596.06, KLD: 151.81), Beta: 4.600, LR: 2.60e-05, ValL: 29068.67 (R: 28444.88, KLD: 135.61)\n",
      "[INFO]   Fold 3/5 VAE E205/300, TrL: 34231.43 (R: 33537.37, KLD: 150.88), Beta: 4.600, LR: 7.32e-06, ValL: 28895.84 (R: 28293.67, KLD: 130.91)\n",
      "[INFO]   Fold 3/5 VAE E210/300, TrL: 34194.08 (R: 33515.40, KLD: 147.54), Beta: 4.600, LR: 1.11e-07, ValL: 28935.90 (R: 28345.24, KLD: 128.41)\n",
      "[INFO]   Fold 3/5 VAE E215/300, TrL: 34277.56 (R: 33567.01, KLD: 154.47), Beta: 4.600, LR: 9.38e-05, ValL: 28982.68 (R: 28370.60, KLD: 133.06)\n",
      "[INFO]   Fold 3/5 VAE E220/300, TrL: 33996.00 (R: 33293.99, KLD: 152.61), Beta: 4.600, LR: 7.59e-05, ValL: 28689.73 (R: 28070.62, KLD: 134.59)\n",
      "[INFO]   Fold 3/5 VAE E225/300, TrL: 33810.91 (R: 33115.27, KLD: 151.23), Beta: 4.600, LR: 5.11e-05, ValL: 28536.23 (R: 27930.51, KLD: 131.68)\n",
      "[INFO]   Fold 3/5 VAE E230/300, TrL: 33039.55 (R: 32930.17, KLD: 178.35), Beta: 0.613, LR: 2.60e-05, ValL: 27955.29 (R: 27855.26, KLD: 163.09)\n",
      "[INFO]   Fold 3/5 VAE E235/300, TrL: 33005.76 (R: 32735.12, KLD: 196.12), Beta: 1.380, LR: 7.32e-06, ValL: 27967.98 (R: 27721.22, KLD: 178.81)\n",
      "[INFO]   Fold 3/5 VAE E240/300, TrL: 33156.69 (R: 32737.58, KLD: 195.24), Beta: 2.147, LR: 1.11e-07, ValL: 28136.82 (R: 27753.42, KLD: 178.60)\n",
      "[INFO]   Fold 3/5 VAE E245/300, TrL: 33316.75 (R: 32759.78, KLD: 191.18), Beta: 2.913, LR: 9.38e-05, ValL: 28279.78 (R: 27776.67, KLD: 172.69)\n",
      "[INFO]   Fold 3/5 VAE E250/300, TrL: 33233.91 (R: 32565.29, KLD: 181.69), Beta: 3.680, LR: 7.59e-05, ValL: 28215.68 (R: 27642.51, KLD: 155.75)\n",
      "[INFO]   Fold 3/5 VAE E255/300, TrL: 33238.86 (R: 32470.96, KLD: 172.69), Beta: 4.447, LR: 5.11e-05, ValL: 28168.01 (R: 27521.84, KLD: 145.32)\n",
      "[INFO]   Fold 3/5 Early stopping VAE en epoch 258. Mejor val_loss: 27872.1824 (época 228)\n",
      "[INFO]   Fold 3/5 VAE final model loaded (best val_loss: 27872.1824).\n",
      "[INFO]   Fold 3/5 Modelo VAE guardado en: resultados_12_inter/fold_3/vae_model_fold_3.pt\n",
      "[INFO] Fold 3/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 74 (50.3%)\n",
      "      M: 73 (49.7%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'xgb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 3.0493386983667325, 'model__n_estimators': 763, 'model__learning_rate': 0.00011154109074942674, 'model__max_depth': 8, 'model__subsample': 0.3021250128595222, 'model__colsample_bytree': 0.5000938884414747, 'model__min_child_weight': 1.086182232922619}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 3 (xgb): AUC=0.8304, Bal.Acc=0.7558\n",
      "[INFO]       Pipeline completo de xgb del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'svm'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 62.88307621324923, 'model__estimator__gamma': 2.937839222388093e-05, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 3 (svm): AUC=0.8187, Bal.Acc=0.7851\n",
      "[INFO]       Pipeline completo de svm del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'logreg'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.004995989031840286}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 3 (logreg): AUC=0.8450, Bal.Acc=0.7558\n",
      "[INFO]       Pipeline completo de logreg del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'gb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 6, 'model__estimator__num_leaves': 762, 'model__estimator__bagging_fraction': 0.6819146796890857, 'model__estimator__feature_fraction': 0.8433542360987833, 'model__estimator__bagging_freq': 5, 'model__estimator__learning_rate': 0.004179085124152246, 'model__estimator__n_estimators': 946, 'model__estimator__min_child_samples': 5, 'model__estimator__min_child_weight': 0.0049212475492562075, 'model__estimator__min_split_gain': 0.7418936073034813, 'model__estimator__reg_alpha': 0.061105974199511484, 'model__estimator__reg_lambda': 0.5772133331655073}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 3 (gb): AUC=0.7807, Bal.Acc=0.7018\n",
      "[INFO]       Pipeline completo de gb del fold 3 guardado.\n",
      "[INFO]   Fold 3/5 completado en 485.98 segundos.\n",
      "[INFO] --- Iniciando Fold 4/5 ---\n",
      "[INFO] Fold 4/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 18 (48.6%)\n",
      "      M: 19 (51.4%)\n",
      "[INFO] Fold 4/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 188 (47.7%)\n",
      "      M: 206 (52.3%)\n",
      "    Age_Group:\n",
      "      0: 99 (25.1%)\n",
      "      1: 96 (24.4%)\n",
      "      2: 98 (24.9%)\n",
      "      3: 101 (25.6%)\n",
      "[INFO]   Fold 4/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 4/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 62 (19.7%)\n",
      "      CN: 56 (17.8%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 150 (47.6%)\n",
      "      M: 165 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 80 (25.4%)\n",
      "      1: 77 (24.4%)\n",
      "      2: 78 (24.8%)\n",
      "      3: 80 (25.4%)\n",
      "[INFO] Fold 4/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 14 (17.7%)\n",
      "      CN: 15 (19.0%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 38 (48.1%)\n",
      "      M: 41 (51.9%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 19 (24.1%)\n",
      "      2: 20 (25.3%)\n",
      "      3: 21 (26.6%)\n",
      "[INFO]   Fold 4/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.047, std=0.775)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.058, std=0.816)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.403, std=1.693)\n",
      "[INFO]   Fold 4/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 4/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 4/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 4/5 VAE E5/300, TrL: 63472.75 (R: 63365.83, KLD: 174.33), Beta: 0.613, LR: 9.38e-05, ValL: 58105.85 (R: 58056.60, KLD: 80.29)\n",
      "[INFO]   Fold 4/5 VAE E10/300, TrL: 60333.20 (R: 59972.60, KLD: 261.30), Beta: 1.380, LR: 7.59e-05, ValL: 53783.78 (R: 53161.69, KLD: 450.79)\n",
      "[INFO]   Fold 4/5 VAE E15/300, TrL: 55225.84 (R: 54294.79, KLD: 433.72), Beta: 2.147, LR: 5.11e-05, ValL: 50399.49 (R: 48816.79, KLD: 737.28)\n",
      "[INFO]   Fold 4/5 VAE E20/300, TrL: 53021.36 (R: 52160.44, KLD: 295.51), Beta: 2.913, LR: 2.60e-05, ValL: 48255.50 (R: 47192.83, KLD: 364.76)\n",
      "[INFO]   Fold 4/5 VAE E25/300, TrL: 52376.50 (R: 51413.98, KLD: 261.55), Beta: 3.680, LR: 7.32e-06, ValL: 47672.72 (R: 46550.30, KLD: 305.01)\n",
      "[INFO]   Fold 4/5 VAE E30/300, TrL: 52411.81 (R: 51287.11, KLD: 252.93), Beta: 4.447, LR: 1.11e-07, ValL: 47791.13 (R: 46482.64, KLD: 294.26)\n",
      "[INFO]   Fold 4/5 VAE E35/300, TrL: 49763.34 (R: 48964.62, KLD: 173.64), Beta: 4.600, LR: 9.38e-05, ValL: 44523.75 (R: 43657.44, KLD: 188.33)\n",
      "[INFO]   Fold 4/5 VAE E40/300, TrL: 47451.30 (R: 46748.02, KLD: 152.89), Beta: 4.600, LR: 7.59e-05, ValL: 41719.07 (R: 40978.23, KLD: 161.05)\n",
      "[INFO]   Fold 4/5 VAE E45/300, TrL: 45560.18 (R: 44927.00, KLD: 137.65), Beta: 4.600, LR: 5.11e-05, ValL: 39787.35 (R: 39156.66, KLD: 137.11)\n",
      "[INFO]   Fold 4/5 VAE E50/300, TrL: 44596.70 (R: 43985.59, KLD: 132.85), Beta: 4.600, LR: 2.60e-05, ValL: 38760.05 (R: 38179.68, KLD: 126.17)\n",
      "[INFO]   Fold 4/5 VAE E55/300, TrL: 44096.14 (R: 43485.69, KLD: 132.71), Beta: 4.600, LR: 7.32e-06, ValL: 38383.92 (R: 37818.32, KLD: 122.96)\n",
      "[INFO]   Fold 4/5 VAE E60/300, TrL: 44177.17 (R: 43578.33, KLD: 130.18), Beta: 4.600, LR: 1.11e-07, ValL: 38283.03 (R: 37734.73, KLD: 119.20)\n",
      "[INFO]   Fold 4/5 VAE E65/300, TrL: 42461.22 (R: 41860.44, KLD: 130.60), Beta: 4.600, LR: 9.38e-05, ValL: 36598.10 (R: 36035.80, KLD: 122.24)\n",
      "[INFO]   Fold 4/5 VAE E70/300, TrL: 41326.66 (R: 40732.94, KLD: 129.07), Beta: 4.600, LR: 7.59e-05, ValL: 35288.35 (R: 34769.49, KLD: 112.80)\n",
      "[INFO]   Fold 4/5 VAE E75/300, TrL: 40486.32 (R: 39886.93, KLD: 130.30), Beta: 4.600, LR: 5.11e-05, ValL: 34625.90 (R: 34092.51, KLD: 115.95)\n",
      "[INFO]   Fold 4/5 VAE E80/300, TrL: 39479.06 (R: 39375.44, KLD: 168.93), Beta: 0.613, LR: 2.60e-05, ValL: 33703.76 (R: 33606.77, KLD: 158.13)\n",
      "[INFO]   Fold 4/5 VAE E85/300, TrL: 39324.26 (R: 39076.01, KLD: 179.90), Beta: 1.380, LR: 7.32e-06, ValL: 33637.90 (R: 33406.34, KLD: 167.79)\n",
      "[INFO]   Fold 4/5 VAE E90/300, TrL: 39420.42 (R: 39033.92, KLD: 180.05), Beta: 2.147, LR: 1.11e-07, ValL: 33826.92 (R: 33463.43, KLD: 169.33)\n",
      "[INFO]   Fold 4/5 VAE E95/300, TrL: 38847.83 (R: 38361.26, KLD: 167.02), Beta: 2.913, LR: 9.38e-05, ValL: 33060.24 (R: 32621.59, KLD: 150.57)\n",
      "[INFO]   Fold 4/5 VAE E100/300, TrL: 38362.86 (R: 37752.80, KLD: 165.78), Beta: 3.680, LR: 7.59e-05, ValL: 32667.16 (R: 32087.36, KLD: 157.55)\n",
      "[INFO]   Fold 4/5 VAE E105/300, TrL: 37887.97 (R: 37202.80, KLD: 154.09), Beta: 4.447, LR: 5.11e-05, ValL: 32133.47 (R: 31554.46, KLD: 130.21)\n",
      "[INFO]   Fold 4/5 VAE E110/300, TrL: 37692.03 (R: 37028.94, KLD: 144.15), Beta: 4.600, LR: 2.60e-05, ValL: 31933.82 (R: 31361.02, KLD: 124.52)\n",
      "[INFO]   Fold 4/5 VAE E115/300, TrL: 37580.84 (R: 36937.76, KLD: 139.80), Beta: 4.600, LR: 7.32e-06, ValL: 31803.76 (R: 31269.11, KLD: 116.23)\n",
      "[INFO]   Fold 4/5 VAE E120/300, TrL: 37542.60 (R: 36892.46, KLD: 141.33), Beta: 4.600, LR: 1.11e-07, ValL: 31756.23 (R: 31228.41, KLD: 114.74)\n",
      "[INFO]   Fold 4/5 VAE E125/300, TrL: 37080.87 (R: 36422.45, KLD: 143.14), Beta: 4.600, LR: 9.38e-05, ValL: 31383.43 (R: 30833.92, KLD: 119.46)\n",
      "[INFO]   Fold 4/5 VAE E130/300, TrL: 36758.06 (R: 36085.08, KLD: 146.30), Beta: 4.600, LR: 7.59e-05, ValL: 31103.08 (R: 30512.36, KLD: 128.42)\n",
      "[INFO]   Fold 4/5 VAE E135/300, TrL: 36321.41 (R: 35669.26, KLD: 141.77), Beta: 4.600, LR: 5.11e-05, ValL: 30646.60 (R: 30129.85, KLD: 112.34)\n",
      "[INFO]   Fold 4/5 VAE E140/300, TrL: 36066.38 (R: 35408.85, KLD: 142.94), Beta: 4.600, LR: 2.60e-05, ValL: 30558.46 (R: 30001.99, KLD: 120.97)\n",
      "[INFO]   Fold 4/5 VAE E145/300, TrL: 36007.40 (R: 35366.79, KLD: 139.26), Beta: 4.600, LR: 7.32e-06, ValL: 30437.40 (R: 29917.07, KLD: 113.11)\n",
      "[INFO]   Fold 4/5 VAE E150/300, TrL: 35937.64 (R: 35295.42, KLD: 139.61), Beta: 4.600, LR: 1.11e-07, ValL: 30384.58 (R: 29862.50, KLD: 113.50)\n",
      "[INFO]   Fold 4/5 VAE E155/300, TrL: 34979.76 (R: 34838.72, KLD: 229.96), Beta: 0.613, LR: 9.38e-05, ValL: 29607.83 (R: 29480.93, KLD: 206.90)\n",
      "[INFO]   Fold 4/5 VAE E160/300, TrL: 34847.60 (R: 34509.00, KLD: 245.36), Beta: 1.380, LR: 7.59e-05, ValL: 29474.94 (R: 29162.56, KLD: 226.36)\n",
      "[INFO]   Fold 4/5 VAE E165/300, TrL: 34657.84 (R: 34153.75, KLD: 234.82), Beta: 2.147, LR: 5.11e-05, ValL: 29412.52 (R: 28958.78, KLD: 211.37)\n",
      "[INFO]   Fold 4/5 VAE E170/300, TrL: 34712.62 (R: 34103.51, KLD: 209.08), Beta: 2.913, LR: 2.60e-05, ValL: 29465.04 (R: 28917.38, KLD: 187.99)\n",
      "[INFO]   Fold 4/5 VAE E175/300, TrL: 34899.62 (R: 34161.61, KLD: 200.55), Beta: 3.680, LR: 7.32e-06, ValL: 29540.30 (R: 28886.86, KLD: 177.56)\n",
      "[INFO]   Fold 4/5 VAE E180/300, TrL: 34866.13 (R: 33992.17, KLD: 196.54), Beta: 4.447, LR: 1.11e-07, ValL: 29595.92 (R: 28832.81, KLD: 171.61)\n",
      "[INFO]   Fold 4/5 VAE E185/300, TrL: 34744.35 (R: 34045.53, KLD: 151.92), Beta: 4.600, LR: 9.38e-05, ValL: 29383.04 (R: 28775.78, KLD: 132.01)\n",
      "[INFO]   Fold 4/5 VAE E190/300, TrL: 34533.18 (R: 33784.27, KLD: 162.80), Beta: 4.600, LR: 7.59e-05, ValL: 29221.85 (R: 28575.97, KLD: 140.41)\n",
      "[INFO]   Fold 4/5 VAE E195/300, TrL: 34334.62 (R: 33582.63, KLD: 163.48), Beta: 4.600, LR: 5.11e-05, ValL: 29108.24 (R: 28485.78, KLD: 135.32)\n",
      "[INFO]   Fold 4/5 VAE E200/300, TrL: 34066.60 (R: 33368.69, KLD: 151.72), Beta: 4.600, LR: 2.60e-05, ValL: 28957.13 (R: 28378.27, KLD: 125.84)\n",
      "[INFO]   Fold 4/5 VAE E205/300, TrL: 34044.16 (R: 33347.23, KLD: 151.51), Beta: 4.600, LR: 7.32e-06, ValL: 28922.14 (R: 28351.17, KLD: 124.12)\n",
      "[INFO]   Fold 4/5 VAE E210/300, TrL: 33958.85 (R: 33257.93, KLD: 152.38), Beta: 4.600, LR: 1.11e-07, ValL: 28901.72 (R: 28332.84, KLD: 123.67)\n",
      "[INFO]   Fold 4/5 VAE E215/300, TrL: 33937.93 (R: 33218.52, KLD: 156.39), Beta: 4.600, LR: 9.38e-05, ValL: 28981.09 (R: 28329.49, KLD: 141.65)\n",
      "[INFO]   Fold 4/5 VAE E220/300, TrL: 33812.92 (R: 33067.85, KLD: 161.97), Beta: 4.600, LR: 7.59e-05, ValL: 28762.37 (R: 28128.90, KLD: 137.71)\n",
      "[INFO]   Fold 4/5 VAE E225/300, TrL: 33623.10 (R: 32899.12, KLD: 157.39), Beta: 4.600, LR: 5.11e-05, ValL: 28580.63 (R: 27969.11, KLD: 132.94)\n",
      "[INFO]   Fold 4/5 VAE E230/300, TrL: 32924.19 (R: 32809.77, KLD: 186.55), Beta: 0.613, LR: 2.60e-05, ValL: 27928.58 (R: 27829.79, KLD: 161.07)\n",
      "[INFO]   Fold 4/5 VAE E235/300, TrL: 32824.34 (R: 32549.70, KLD: 199.01), Beta: 1.380, LR: 7.32e-06, ValL: 27974.87 (R: 27734.38, KLD: 174.27)\n",
      "[INFO]   Fold 4/5 VAE E240/300, TrL: 32968.26 (R: 32543.88, KLD: 197.69), Beta: 2.147, LR: 1.11e-07, ValL: 28113.09 (R: 27739.26, KLD: 174.15)\n",
      "[INFO]   Fold 4/5 VAE E245/300, TrL: 33055.99 (R: 32485.18, KLD: 195.93), Beta: 2.913, LR: 9.38e-05, ValL: 28200.81 (R: 27704.28, KLD: 170.43)\n",
      "[INFO]   Fold 4/5 VAE E250/300, TrL: 32971.20 (R: 32286.00, KLD: 186.20), Beta: 3.680, LR: 7.59e-05, ValL: 28179.17 (R: 27599.31, KLD: 157.57)\n",
      "[INFO]   Fold 4/5 VAE E255/300, TrL: 32994.15 (R: 32217.04, KLD: 174.76), Beta: 4.447, LR: 5.11e-05, ValL: 28207.98 (R: 27570.80, KLD: 143.29)\n",
      "[INFO]   Fold 4/5 Early stopping VAE en epoch 258. Mejor val_loss: 27920.9632 (época 228)\n",
      "[INFO]   Fold 4/5 VAE final model loaded (best val_loss: 27920.9632).\n",
      "[INFO]   Fold 4/5 Modelo VAE guardado en: resultados_12_inter/fold_4/vae_model_fold_4.pt\n",
      "[INFO] Fold 4/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 74 (50.3%)\n",
      "      M: 73 (49.7%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'xgb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 3.9260713433722265, 'model__n_estimators': 974, 'model__learning_rate': 0.00745045302749974, 'model__max_depth': 9, 'model__subsample': 0.31436913425990987, 'model__colsample_bytree': 0.6477992909904485, 'model__min_child_weight': 0.6312775591455694}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 4 (xgb): AUC=0.7865, Bal.Acc=0.7822\n",
      "[INFO]       Pipeline completo de xgb del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'svm'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 223.96723770762128, 'model__estimator__gamma': 2.102532124113119e-05, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 4 (svm): AUC=0.7924, Bal.Acc=0.7807\n",
      "[INFO]       Pipeline completo de svm del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'logreg'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.012470018071834208}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 4 (logreg): AUC=0.7778, Bal.Acc=0.7281\n",
      "[INFO]       Pipeline completo de logreg del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'gb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 7, 'model__estimator__num_leaves': 23, 'model__estimator__bagging_fraction': 0.6151598763550042, 'model__estimator__feature_fraction': 0.5649243952046877, 'model__estimator__bagging_freq': 9, 'model__estimator__learning_rate': 0.0023578563609731027, 'model__estimator__n_estimators': 944, 'model__estimator__min_child_samples': 5, 'model__estimator__min_child_weight': 0.10104145454126476, 'model__estimator__min_split_gain': 0.9930728830037885, 'model__estimator__reg_alpha': 0.0010989568762361077, 'model__estimator__reg_lambda': 0.016415875567598617}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 4 (gb): AUC=0.7734, Bal.Acc=0.7295\n",
      "[INFO]       Pipeline completo de gb del fold 4 guardado.\n",
      "[INFO]   Fold 4/5 completado en 576.17 segundos.\n",
      "[INFO] --- Iniciando Fold 5/5 ---\n",
      "[INFO] Fold 5/5 Test Set (Clasificador) (N=36):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (52.8%)\n",
      "      CN: 17 (47.2%)\n",
      "    Sex:\n",
      "      F: 18 (50.0%)\n",
      "      M: 18 (50.0%)\n",
      "[INFO] Fold 5/5 Pool Entrenamiento VAE (N=395):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.2%)\n",
      "      CN: 72 (18.2%)\n",
      "      MCI: 247 (62.5%)\n",
      "    Sex:\n",
      "      F: 188 (47.6%)\n",
      "      M: 207 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 101 (25.6%)\n",
      "      1: 105 (26.6%)\n",
      "      2: 98 (24.8%)\n",
      "      3: 91 (23.0%)\n",
      "[INFO]   Fold 5/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 5/5 Actual Train Set (VAE) (N=316):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 61 (19.3%)\n",
      "      CN: 58 (18.4%)\n",
      "      MCI: 197 (62.3%)\n",
      "    Sex:\n",
      "      F: 151 (47.8%)\n",
      "      M: 165 (52.2%)\n",
      "    Age_Group:\n",
      "      0: 82 (25.9%)\n",
      "      1: 84 (26.6%)\n",
      "      2: 77 (24.4%)\n",
      "      3: 73 (23.1%)\n",
      "[INFO] Fold 5/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 15 (19.0%)\n",
      "      CN: 14 (17.7%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 37 (46.8%)\n",
      "      M: 42 (53.2%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 21 (26.6%)\n",
      "      2: 21 (26.6%)\n",
      "      3: 18 (22.8%)\n",
      "[INFO]   Fold 5/5 Sujetos VAE actual train: 316, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 316 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.047, std=0.777)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.057, std=0.814)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.407, std=1.702)\n",
      "[INFO]   Fold 5/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 5/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 5/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 5/5 VAE E5/300, TrL: 63360.53 (R: 63252.00, KLD: 176.96), Beta: 0.613, LR: 9.38e-05, ValL: 61914.65 (R: 61861.86, KLD: 86.06)\n",
      "[INFO]   Fold 5/5 VAE E10/300, TrL: 59044.09 (R: 58596.76, KLD: 324.15), Beta: 1.380, LR: 7.59e-05, ValL: 56504.36 (R: 55646.62, KLD: 621.55)\n",
      "[INFO]   Fold 5/5 VAE E15/300, TrL: 53899.70 (R: 52992.37, KLD: 422.67), Beta: 2.147, LR: 5.11e-05, ValL: 53089.52 (R: 51855.10, KLD: 575.04)\n",
      "[INFO]   Fold 5/5 VAE E20/300, TrL: 51722.22 (R: 50875.15, KLD: 290.75), Beta: 2.913, LR: 2.60e-05, ValL: 50827.95 (R: 49898.79, KLD: 318.93)\n",
      "[INFO]   Fold 5/5 VAE E25/300, TrL: 51144.76 (R: 50185.42, KLD: 260.69), Beta: 3.680, LR: 7.32e-06, ValL: 50269.86 (R: 49212.77, KLD: 287.25)\n",
      "[INFO]   Fold 5/5 VAE E30/300, TrL: 51175.15 (R: 50039.51, KLD: 255.39), Beta: 4.447, LR: 1.11e-07, ValL: 50360.24 (R: 49132.08, KLD: 276.20)\n",
      "[INFO]   Fold 5/5 VAE E35/300, TrL: 48152.92 (R: 47350.35, KLD: 174.47), Beta: 4.600, LR: 9.38e-05, ValL: 46736.98 (R: 45907.39, KLD: 180.35)\n",
      "[INFO]   Fold 5/5 VAE E40/300, TrL: 45772.24 (R: 45089.67, KLD: 148.38), Beta: 4.600, LR: 7.59e-05, ValL: 44395.68 (R: 43675.65, KLD: 156.53)\n",
      "[INFO]   Fold 5/5 VAE E45/300, TrL: 44256.64 (R: 43617.09, KLD: 139.03), Beta: 4.600, LR: 5.11e-05, ValL: 42778.77 (R: 42153.85, KLD: 135.85)\n",
      "[INFO]   Fold 5/5 VAE E50/300, TrL: 43514.15 (R: 42891.50, KLD: 135.36), Beta: 4.600, LR: 2.60e-05, ValL: 41991.67 (R: 41423.36, KLD: 123.55)\n",
      "[INFO]   Fold 5/5 VAE E55/300, TrL: 43217.44 (R: 42614.72, KLD: 131.03), Beta: 4.600, LR: 7.32e-06, ValL: 41654.88 (R: 41120.00, KLD: 116.28)\n",
      "[INFO]   Fold 5/5 VAE E60/300, TrL: 43144.47 (R: 42543.34, KLD: 130.68), Beta: 4.600, LR: 1.11e-07, ValL: 41599.91 (R: 41077.58, KLD: 113.55)\n",
      "[INFO]   Fold 5/5 VAE E65/300, TrL: 41882.67 (R: 41262.12, KLD: 134.90), Beta: 4.600, LR: 9.38e-05, ValL: 40334.17 (R: 39736.58, KLD: 129.91)\n",
      "[INFO]   Fold 5/5 VAE E70/300, TrL: 40737.06 (R: 40149.59, KLD: 127.71), Beta: 4.600, LR: 7.59e-05, ValL: 39316.96 (R: 38775.18, KLD: 117.78)\n",
      "[INFO]   Fold 5/5 VAE E75/300, TrL: 40186.24 (R: 39574.16, KLD: 133.06), Beta: 4.600, LR: 5.11e-05, ValL: 38627.31 (R: 38120.25, KLD: 110.23)\n",
      "[INFO]   Fold 5/5 VAE E80/300, TrL: 39086.91 (R: 38983.06, KLD: 169.31), Beta: 0.613, LR: 2.60e-05, ValL: 37875.54 (R: 37777.28, KLD: 160.21)\n",
      "[INFO]   Fold 5/5 VAE E85/300, TrL: 39118.35 (R: 38867.20, KLD: 181.99), Beta: 1.380, LR: 7.32e-06, ValL: 37786.24 (R: 37548.88, KLD: 172.00)\n",
      "[INFO]   Fold 5/5 VAE E90/300, TrL: 39164.36 (R: 38775.56, KLD: 181.12), Beta: 2.147, LR: 1.11e-07, ValL: 37938.81 (R: 37569.50, KLD: 172.04)\n",
      "[INFO]   Fold 5/5 VAE E95/300, TrL: 38726.26 (R: 38228.16, KLD: 170.97), Beta: 2.913, LR: 9.38e-05, ValL: 37358.83 (R: 36879.26, KLD: 164.61)\n",
      "[INFO]   Fold 5/5 VAE E100/300, TrL: 38164.37 (R: 37563.90, KLD: 163.17), Beta: 3.680, LR: 7.59e-05, ValL: 36814.81 (R: 36288.61, KLD: 142.99)\n",
      "[INFO]   Fold 5/5 VAE E105/300, TrL: 37781.57 (R: 37125.52, KLD: 147.54), Beta: 4.447, LR: 5.11e-05, ValL: 36501.33 (R: 35942.80, KLD: 125.61)\n",
      "[INFO]   Fold 5/5 VAE E110/300, TrL: 37468.34 (R: 36812.54, KLD: 142.57), Beta: 4.600, LR: 2.60e-05, ValL: 36324.83 (R: 35750.96, KLD: 124.75)\n",
      "[INFO]   Fold 5/5 VAE E115/300, TrL: 37438.10 (R: 36803.20, KLD: 138.02), Beta: 4.600, LR: 7.32e-06, ValL: 36149.51 (R: 35608.54, KLD: 117.60)\n",
      "[INFO]   Fold 5/5 VAE E120/300, TrL: 37418.26 (R: 36790.61, KLD: 136.45), Beta: 4.600, LR: 1.11e-07, ValL: 36164.80 (R: 35634.10, KLD: 115.37)\n",
      "[INFO]   Fold 5/5 VAE E125/300, TrL: 37011.33 (R: 36364.49, KLD: 140.62), Beta: 4.600, LR: 9.38e-05, ValL: 35817.98 (R: 35203.91, KLD: 133.49)\n",
      "[INFO]   Fold 5/5 VAE E130/300, TrL: 36691.02 (R: 36049.36, KLD: 139.49), Beta: 4.600, LR: 7.59e-05, ValL: 35337.98 (R: 34768.75, KLD: 123.75)\n",
      "[INFO]   Fold 5/5 VAE E135/300, TrL: 36289.47 (R: 35646.08, KLD: 139.87), Beta: 4.600, LR: 5.11e-05, ValL: 35036.70 (R: 34505.15, KLD: 115.55)\n",
      "[INFO]   Fold 5/5 VAE E140/300, TrL: 35999.22 (R: 35376.64, KLD: 135.34), Beta: 4.600, LR: 2.60e-05, ValL: 34906.50 (R: 34380.66, KLD: 114.31)\n",
      "[INFO]   Fold 5/5 VAE E145/300, TrL: 35891.19 (R: 35264.16, KLD: 136.31), Beta: 4.600, LR: 7.32e-06, ValL: 34812.75 (R: 34295.37, KLD: 112.47)\n",
      "[INFO]   Fold 5/5 VAE E150/300, TrL: 35921.67 (R: 35294.97, KLD: 136.24), Beta: 4.600, LR: 1.11e-07, ValL: 34818.70 (R: 34302.93, KLD: 112.12)\n",
      "[INFO]   Fold 5/5 VAE E155/300, TrL: 34985.34 (R: 34849.86, KLD: 220.88), Beta: 0.613, LR: 9.38e-05, ValL: 34040.43 (R: 33918.04, KLD: 199.56)\n",
      "[INFO]   Fold 5/5 VAE E160/300, TrL: 34787.05 (R: 34448.10, KLD: 245.62), Beta: 1.380, LR: 7.59e-05, ValL: 33910.33 (R: 33592.46, KLD: 230.34)\n",
      "[INFO]   Fold 5/5 VAE E165/300, TrL: 34687.37 (R: 34205.36, KLD: 224.54), Beta: 2.147, LR: 5.11e-05, ValL: 33796.65 (R: 33361.60, KLD: 202.67)\n",
      "[INFO]   Fold 5/5 VAE E170/300, TrL: 34693.10 (R: 34096.23, KLD: 204.88), Beta: 2.913, LR: 2.60e-05, ValL: 33830.16 (R: 33296.04, KLD: 183.34)\n",
      "[INFO]   Fold 5/5 VAE E175/300, TrL: 34768.64 (R: 34049.52, KLD: 195.41), Beta: 3.680, LR: 7.32e-06, ValL: 33886.65 (R: 33253.28, KLD: 172.11)\n",
      "[INFO]   Fold 5/5 VAE E180/300, TrL: 34831.57 (R: 33969.53, KLD: 193.86), Beta: 4.447, LR: 1.11e-07, ValL: 34002.30 (R: 33249.62, KLD: 169.27)\n",
      "[INFO]   Fold 5/5 VAE E185/300, TrL: 34706.56 (R: 34012.60, KLD: 150.86), Beta: 4.600, LR: 9.38e-05, ValL: 33775.92 (R: 33173.49, KLD: 130.96)\n",
      "[INFO]   Fold 5/5 VAE E190/300, TrL: 34409.85 (R: 33695.73, KLD: 155.24), Beta: 4.600, LR: 7.59e-05, ValL: 33546.34 (R: 32947.14, KLD: 130.26)\n",
      "[INFO]   Fold 5/5 VAE E195/300, TrL: 34335.34 (R: 33621.13, KLD: 155.26), Beta: 4.600, LR: 5.11e-05, ValL: 33435.19 (R: 32848.75, KLD: 127.49)\n",
      "[INFO]   Fold 5/5 VAE E200/300, TrL: 34144.63 (R: 33448.68, KLD: 151.29), Beta: 4.600, LR: 2.60e-05, ValL: 33306.31 (R: 32734.95, KLD: 124.21)\n",
      "[INFO]   Fold 5/5 VAE E205/300, TrL: 34082.76 (R: 33403.28, KLD: 147.71), Beta: 4.600, LR: 7.32e-06, ValL: 33193.80 (R: 32644.84, KLD: 119.34)\n",
      "[INFO]   Fold 5/5 VAE E210/300, TrL: 34032.90 (R: 33356.93, KLD: 146.95), Beta: 4.600, LR: 1.11e-07, ValL: 33266.04 (R: 32720.26, KLD: 118.65)\n",
      "[INFO]   Fold 5/5 VAE E215/300, TrL: 33858.19 (R: 33153.92, KLD: 153.10), Beta: 4.600, LR: 9.38e-05, ValL: 33239.55 (R: 32683.66, KLD: 120.85)\n",
      "[INFO]   Fold 5/5 VAE E220/300, TrL: 33746.43 (R: 33042.48, KLD: 153.03), Beta: 4.600, LR: 7.59e-05, ValL: 32964.06 (R: 32386.95, KLD: 125.46)\n",
      "[INFO]   Fold 5/5 VAE E225/300, TrL: 33674.65 (R: 32975.97, KLD: 151.89), Beta: 4.600, LR: 5.11e-05, ValL: 32766.74 (R: 32210.51, KLD: 120.92)\n",
      "[INFO]   Fold 5/5 VAE E230/300, TrL: 32801.54 (R: 32692.28, KLD: 178.16), Beta: 0.613, LR: 2.60e-05, ValL: 32210.40 (R: 32114.92, KLD: 155.67)\n",
      "[INFO]   Fold 5/5 VAE E235/300, TrL: 32807.84 (R: 32542.14, KLD: 192.54), Beta: 1.380, LR: 7.32e-06, ValL: 32293.36 (R: 32066.28, KLD: 164.55)\n",
      "[INFO]   Fold 5/5 VAE E240/300, TrL: 32993.62 (R: 32578.62, KLD: 193.32), Beta: 2.147, LR: 1.11e-07, ValL: 32395.30 (R: 32036.69, KLD: 167.05)\n",
      "[INFO]   Fold 5/5 VAE E245/300, TrL: 33109.93 (R: 32551.06, KLD: 191.83), Beta: 2.913, LR: 9.38e-05, ValL: 32496.67 (R: 31998.37, KLD: 171.04)\n",
      "[INFO]   Fold 5/5 VAE E250/300, TrL: 32984.51 (R: 32312.03, KLD: 182.74), Beta: 3.680, LR: 7.59e-05, ValL: 32480.53 (R: 31893.92, KLD: 159.40)\n",
      "[INFO]   Fold 5/5 VAE E255/300, TrL: 32963.15 (R: 32204.57, KLD: 170.60), Beta: 4.447, LR: 5.11e-05, ValL: 32451.23 (R: 31825.49, KLD: 140.72)\n",
      "[INFO]   Fold 5/5 Early stopping VAE en epoch 258. Mejor val_loss: 32169.3436 (época 228)\n",
      "[INFO]   Fold 5/5 VAE final model loaded (best val_loss: 32169.3436).\n",
      "[INFO]   Fold 5/5 Modelo VAE guardado en: resultados_12_inter/fold_5/vae_model_fold_5.pt\n",
      "[INFO] Fold 5/5 Pool Train/Dev (Clasificador) (N=148):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.4%)\n",
      "      CN: 72 (48.6%)\n",
      "    Sex:\n",
      "      F: 74 (50.0%)\n",
      "      M: 74 (50.0%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (148, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'xgb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 4.174019512891148, 'model__n_estimators': 1339, 'model__learning_rate': 0.0017378616945608319, 'model__max_depth': 11, 'model__subsample': 0.9951882626981414, 'model__colsample_bytree': 0.9126757284501635, 'model__min_child_weight': 0.5601929491927373}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 5 (xgb): AUC=0.7368, Bal.Acc=0.7012\n",
      "[INFO]       Pipeline completo de xgb del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'svm'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 1.2897693817666334, 'model__estimator__gamma': 0.00023689217921849665, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 5 (svm): AUC=0.7477, Bal.Acc=0.6981\n",
      "[INFO]       Pipeline completo de svm del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'logreg'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.03531953826668321}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 5 (logreg): AUC=0.7709, Bal.Acc=0.6950\n",
      "[INFO]       Pipeline completo de logreg del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ Usando implementación de imblearn (CPU) para el clasificador 'gb'.\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 9, 'model__estimator__num_leaves': 169, 'model__estimator__bagging_fraction': 0.5082875041521525, 'model__estimator__feature_fraction': 0.746848764361898, 'model__estimator__bagging_freq': 1, 'model__estimator__learning_rate': 0.0006183826254472981, 'model__estimator__n_estimators': 483, 'model__estimator__min_child_samples': 6, 'model__estimator__min_child_weight': 0.04605783784772002, 'model__estimator__min_split_gain': 0.524260065371462, 'model__estimator__reg_alpha': 0.5755627311012296, 'model__estimator__reg_lambda': 0.28872981638756373}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 5 (gb): AUC=0.6780, Bal.Acc=0.6920\n",
      "[INFO]       Pipeline completo de gb del fold 5 guardado.\n",
      "[INFO]   Fold 5/5 completado en 487.01 segundos.\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: xgb (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.8152 +/- 0.0612\n",
      "[INFO] Pr_auc              : 0.8153 +/- 0.0755\n",
      "[INFO] Accuracy            : 0.7497 +/- 0.0382\n",
      "[INFO] Balanced_accuracy   : 0.7496 +/- 0.0349\n",
      "[INFO] Sensitivity         : 0.7789 +/- 0.1200\n",
      "[INFO] Specificity         : 0.7203 +/- 0.0641\n",
      "[INFO] F1_score            : 0.7591 +/- 0.0569\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: svm (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.7966 +/- 0.0766\n",
      "[INFO] Pr_auc              : 0.7824 +/- 0.0862\n",
      "[INFO] Accuracy            : 0.7389 +/- 0.0957\n",
      "[INFO] Balanced_accuracy   : 0.7385 +/- 0.0965\n",
      "[INFO] Sensitivity         : 0.7684 +/- 0.1026\n",
      "[INFO] Specificity         : 0.7085 +/- 0.1626\n",
      "[INFO] F1_score            : 0.7533 +/- 0.0816\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: logreg (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.8168 +/- 0.0472\n",
      "[INFO] Pr_auc              : 0.8102 +/- 0.0621\n",
      "[INFO] Accuracy            : 0.7497 +/- 0.0382\n",
      "[INFO] Balanced_accuracy   : 0.7495 +/- 0.0384\n",
      "[INFO] Sensitivity         : 0.7579 +/- 0.0706\n",
      "[INFO] Specificity         : 0.7412 +/- 0.0856\n",
      "[INFO] F1_score            : 0.7573 +/- 0.0355\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: gb (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.8014 +/- 0.0937\n",
      "[INFO] Pr_auc              : 0.7905 +/- 0.1060\n",
      "[INFO] Accuracy            : 0.7497 +/- 0.0811\n",
      "[INFO] Balanced_accuracy   : 0.7486 +/- 0.0824\n",
      "[INFO] Sensitivity         : 0.7789 +/- 0.0577\n",
      "[INFO] Specificity         : 0.7183 +/- 0.1327\n",
      "[INFO] F1_score            : 0.7647 +/- 0.0717\n",
      "[INFO] Resultados detallados de todos los clasificadores guardados en: resultados_12_inter/all_folds_metrics_MULTI_xgb_vaeconvtranspose4l_ld512_beta4.6_normzscore_offdiag_ch3sel_intFCquarter_drop0.2_ln0_outer5x1_scoreroc_auc.csv\n",
      "[INFO] Sumario estadístico de métricas (por clasificador) guardado en: resultados_12_inter/summary_metrics_MULTI_xgb_vaeconvtranspose4l_ld512_beta4.6_normzscore_offdiag_ch3sel_intFCquarter_drop0.2_ln0_outer5x1_scoreroc_auc.txt\n",
      "[INFO] Pipeline completo en 2304.65 segundos.\n",
      "[INFO] --- Consideraciones Finales ---\n",
      "[INFO] Normalización: 'zscore_offdiag'. Activación VAE: 'tanh'. Asegurar compatibilidad.\n"
     ]
    }
   ],
   "source": [
    "!python serentipia8.py \\\n",
    "  --global_tensor_path /home/diego/Escritorio/limpio/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz \\\n",
    "  --metadata_path /home/diego/Escritorio/limpio/SubjectsData_AAL3_procesado.csv \\\n",
    "  --roi_order_path /home/diego/Escritorio/limpio/roi_order_131.npy \\\n",
    "  --output_dir ./resultados_12_inter \\\n",
    "  --channels_to_use 1 2 5 \\\n",
    "  --classifier_types xgb svm logreg gb \\\n",
    "  --classifier_calibrate \\\n",
    "  --outer_folds 5 \\\n",
    "  --repeated_outer_folds_n_repeats 1 \\\n",
    "  --epochs_vae 300 \\\n",
    "  --early_stopping_patience_vae 30 \\\n",
    "  --cyclical_beta_n_cycles 4 \\\n",
    "  --lr_scheduler_type cosine_warm --lr_scheduler_T0 30 \\\n",
    "  --batch_size 64 \\\n",
    "  --beta_vae 4.6 \\\n",
    "  --latent_dim 512 \\\n",
    "  --n_jobs_gridsearch 8 \\\n",
    "  --metadata_features Age Sex \\\n",
    "  --use_smote \\\n",
    "  --use_optuna_pruner \\\n",
    "  --classifier_use_class_weight \\\n",
    "  --save_fold_artefacts \\\n",
    "  --gridsearch_scoring roc_auc \\\n",
    "  --save_vae_training_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
